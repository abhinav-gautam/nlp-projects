{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob import Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading file\n",
    "file = open(\"article.txt\",'r')\n",
    "file_content = TextBlob(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stopwords\n",
    "from nltk.corpus import stopwords\n",
    "without_stopword = (word for word in file_content.words if word not in stopwords.words('english'))\n",
    "without_stopword = \" \".join(without_stopword)\n",
    "file_content = TextBlob(without_stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting tags and word count\n",
    "file_tags = file_content.pos_tags\n",
    "file_word_count = file_content.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting most repeated unique nouns\n",
    "from functools import reduce\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "\n",
    "def custom_reduce(obj1, obj2):\n",
    "    return (obj1[0],obj1[1],(obj1[2]+obj2[2]))\n",
    "\n",
    "word_tag_count = list()\n",
    "for word,tag in set(file_tags):\n",
    "    if tag in [\"NNP\",\"NNS\"]:\n",
    "        word_tag_count.append((word,tag,file_content.words.count(word)))\n",
    "        \n",
    "word_tag_count_lower = [(word.lower(),tag,count) for word,tag,count in word_tag_count]\n",
    "word_tag_count_lower_unique = [reduce(custom_reduce, group) for _, group in groupby(sorted(word_tag_count_lower), key=itemgetter(0))]\n",
    "word_tag_count_lower_unique_sorted = sorted(word_tag_count_lower_unique,key=lambda x:x[2],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paragraph is about 'Internet, Data and Exchange'.\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "print(\"The paragraph is about '\"\n",
    "      +word_tag_count_lower_unique_sorted[0][0].title()\n",
    "      +\", \"\n",
    "      +word_tag_count_lower_unique_sorted[1][0].title()\n",
    "      +\" and \"\n",
    "      +word_tag_count_lower_unique_sorted[2][0].title()\n",
    "      +\"'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 ('data-science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "1ba2b7840eef77421e06f337bfc5ba96d44c69d7e81c0f8040b4451e2c00e338"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
